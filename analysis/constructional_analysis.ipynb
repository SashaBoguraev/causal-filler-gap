{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction Specific Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be doing the analysis to find construction - specific effects. As a reminder, this consists of three main parts:\n",
    "\n",
    "1. Crafting our transfer matrix\n",
    "2. Network Analysis\n",
    "3. Asymmetry Analysis/Tiering\n",
    "\n",
    "First, however, let us load in the data we are interested in. For now this will just be the single-clause single-construction generalization data. However, we eventually want to expand to the multi-clause setting too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = False\n",
    "size = \"1.4b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(f\"../results/generalization/embedded_clause_single_construction_classic_{size}.parquet\") if embedded else \\\n",
    "    pd.read_parquet(f\"../results/generalization/single_clause_single_construction_classic_{size}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Crafting the Transfer Matrix\n",
    "\n",
    "In this section, we will craft the transfer matrix. This is a $(n+2) * (n+2)$ matrix, in which the rows and columns represent the $n$ constructions and $2$ controls we are evaluating the transfer into. We construct this matrix below.\n",
    "\n",
    "### Step One: Filter by cases where the animacy conditions are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedded:\n",
    "    data = data[(data[\"animacy_condition\"] != \"control\") | \n",
    "                (data[\"animacy_condition\"] != \"control_lexical\")]\n",
    "else:\n",
    "    data = data[(data[\"animacy_condition\"] != \"embedded_control\") | \n",
    "                (data[\"animacy_condition\"] != \"embedded_control_lexical\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Two: Average the values by parent construction.\n",
    "This means that the item representing, for example, *Embedded Finite Wh - Questions* → *Embedded Non-Finite Wh - Questions* will be equal to the average value of :\n",
    "1. Animate Embedded Finite Wh - Questions → Animate Embedded Non-Finite Wh - Questions \n",
    "2. Inanimate Embedded Finite Wh - Questions → inanimate Embedded Non-Finite Wh - Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>model</th>\n",
       "      <th>seed</th>\n",
       "      <th>leave_out</th>\n",
       "      <th>single_double</th>\n",
       "      <th>max_avg</th>\n",
       "      <th>animate_from</th>\n",
       "      <th>animate_to</th>\n",
       "      <th>parent_construction_from</th>\n",
       "      <th>parent_construction_to</th>\n",
       "      <th>animacy_condition</th>\n",
       "      <th>normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{filler}</td>\n",
       "      <td>cleft_animate</td>\n",
       "      <td>cleft_animate</td>\n",
       "      <td>pythia-1.4b</td>\n",
       "      <td>41</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.988794</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>cleft</td>\n",
       "      <td>cleft</td>\n",
       "      <td>SameDataset_SameAnimacy</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{filler}</td>\n",
       "      <td>cleft_animate</td>\n",
       "      <td>cleft_inanimate</td>\n",
       "      <td>pythia-1.4b</td>\n",
       "      <td>41</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3.781297</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>cleft</td>\n",
       "      <td>cleft</td>\n",
       "      <td>SameDataset_DiffAnimacy</td>\n",
       "      <td>0.947980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{filler}</td>\n",
       "      <td>cleft_animate</td>\n",
       "      <td>control</td>\n",
       "      <td>pythia-1.4b</td>\n",
       "      <td>41</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.138282</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>cleft</td>\n",
       "      <td>control</td>\n",
       "      <td>control</td>\n",
       "      <td>0.034668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{filler}</td>\n",
       "      <td>cleft_animate</td>\n",
       "      <td>control_lexical</td>\n",
       "      <td>pythia-1.4b</td>\n",
       "      <td>41</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.040439</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>cleft</td>\n",
       "      <td>control_lexical</td>\n",
       "      <td>control_lexical</td>\n",
       "      <td>0.010138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{filler}</td>\n",
       "      <td>cleft_animate</td>\n",
       "      <td>embedded_wh_finite_animate</td>\n",
       "      <td>pythia-1.4b</td>\n",
       "      <td>41</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.799119</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>cleft</td>\n",
       "      <td>embedded_wh_finite</td>\n",
       "      <td>DiffDataset_SameAnimacy</td>\n",
       "      <td>0.200341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pos           from                          to        model seed  \\\n",
       "0  {filler}  cleft_animate               cleft_animate  pythia-1.4b   41   \n",
       "1  {filler}  cleft_animate             cleft_inanimate  pythia-1.4b   41   \n",
       "2  {filler}  cleft_animate                     control  pythia-1.4b   41   \n",
       "3  {filler}  cleft_animate             control_lexical  pythia-1.4b   41   \n",
       "4  {filler}  cleft_animate  embedded_wh_finite_animate  pythia-1.4b   41   \n",
       "\n",
       "   leave_out  single_double   max_avg  animate_from  animate_to  \\\n",
       "0      False          False  3.988794          True        True   \n",
       "1      False          False  3.781297          True       False   \n",
       "2      False          False  0.138282          True        True   \n",
       "3      False          False  0.040439          True        True   \n",
       "4      False          False  0.799119          True        True   \n",
       "\n",
       "  parent_construction_from parent_construction_to        animacy_condition  \\\n",
       "0                    cleft                  cleft  SameDataset_SameAnimacy   \n",
       "1                    cleft                  cleft  SameDataset_DiffAnimacy   \n",
       "2                    cleft                control                  control   \n",
       "3                    cleft        control_lexical          control_lexical   \n",
       "4                    cleft     embedded_wh_finite  DiffDataset_SameAnimacy   \n",
       "\n",
       "     normal  \n",
       "0  1.000000  \n",
       "1  0.947980  \n",
       "2  0.034668  \n",
       "3  0.010138  \n",
       "4  0.200341  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_parents = pd.unique(data[\"parent_construction_from\"])\n",
    "to_parents = pd.unique(data[\"parent_construction_to\"])\n",
    "unique_pos = pd.unique(data[\"pos\"])\n",
    "\n",
    "transfer_data = []\n",
    "\n",
    "for pos in unique_pos:\n",
    "    for f_parent in from_parents:\n",
    "        for t_parent in to_parents:\n",
    "            filtered_data = data[(data[\"pos\"] == pos) & \n",
    "                                 (data[\"parent_construction_from\"] == f_parent) & \n",
    "                                 (data[\"parent_construction_to\"] == t_parent)]\n",
    "            \n",
    "            if not filtered_data.empty:\n",
    "                normal_val = filtered_data[\"normal\"].mean()\n",
    "                normal_std = filtered_data[\"normal\"].std()\n",
    "                \n",
    "                transfer_data.append({\n",
    "                    \"pos\": pos,\n",
    "                    \"parent_from\": f_parent,\n",
    "                    \"parent_to\": t_parent,\n",
    "                    \"mean_normal\": normal_val,\n",
    "                    \"std_normal\": normal_std,\n",
    "                    \"data_points\": filtered_data[\"normal\"].tolist(),\n",
    "                    \"n_samples\": len(filtered_data)\n",
    "                })\n",
    "\n",
    "transfer_df = pd.DataFrame(transfer_data)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transfer_df is our transfer matrix! Let's take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>parent_from</th>\n",
       "      <th>parent_to</th>\n",
       "      <th>mean_normal</th>\n",
       "      <th>std_normal</th>\n",
       "      <th>data_points</th>\n",
       "      <th>n_samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{filler}</td>\n",
       "      <td>cleft</td>\n",
       "      <td>cleft</td>\n",
       "      <td>0.945206</td>\n",
       "      <td>0.078820</td>\n",
       "      <td>[1.0, 0.9479801325466016, 0.8328432313463736, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{filler}</td>\n",
       "      <td>cleft</td>\n",
       "      <td>control</td>\n",
       "      <td>0.046139</td>\n",
       "      <td>0.016223</td>\n",
       "      <td>[0.03466758061428266, 0.057610572646662375]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{filler}</td>\n",
       "      <td>cleft</td>\n",
       "      <td>control_lexical</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>[0.010138166441087537, 0.008125406540241074]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{filler}</td>\n",
       "      <td>cleft</td>\n",
       "      <td>embedded_wh_finite</td>\n",
       "      <td>0.089693</td>\n",
       "      <td>0.078533</td>\n",
       "      <td>[0.2003410966874174, 0.03012053132554714, 0.09...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{filler}</td>\n",
       "      <td>cleft</td>\n",
       "      <td>embedded_wh_nonfinite</td>\n",
       "      <td>0.057269</td>\n",
       "      <td>0.068639</td>\n",
       "      <td>[0.15624271390433356, 0.007943775647185933, 0....</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>{verb}</td>\n",
       "      <td>wh_question</td>\n",
       "      <td>embedded_wh_nonfinite</td>\n",
       "      <td>0.211237</td>\n",
       "      <td>0.110127</td>\n",
       "      <td>[0.2557628494508001, 0.06806425463981211, 0.19...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>{verb}</td>\n",
       "      <td>wh_question</td>\n",
       "      <td>pseudo_cleft</td>\n",
       "      <td>0.841505</td>\n",
       "      <td>0.244043</td>\n",
       "      <td>[1.0384959650777246, 0.6163913357317051, 0.644...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>{verb}</td>\n",
       "      <td>wh_question</td>\n",
       "      <td>restricted_rc</td>\n",
       "      <td>0.224899</td>\n",
       "      <td>0.137247</td>\n",
       "      <td>[0.23430738971416312, 0.06704753584215735, 0.1...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>{verb}</td>\n",
       "      <td>wh_question</td>\n",
       "      <td>topicalization</td>\n",
       "      <td>0.744543</td>\n",
       "      <td>0.252308</td>\n",
       "      <td>[0.9495505102545027, 0.5333496301381966, 0.519...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>{verb}</td>\n",
       "      <td>wh_question</td>\n",
       "      <td>wh_question</td>\n",
       "      <td>0.809839</td>\n",
       "      <td>0.225532</td>\n",
       "      <td>[1.0, 0.5566341806838216, 0.6827210821389671, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pos  parent_from              parent_to  mean_normal  std_normal  \\\n",
       "0    {filler}        cleft                  cleft     0.945206    0.078820   \n",
       "1    {filler}        cleft                control     0.046139    0.016223   \n",
       "2    {filler}        cleft        control_lexical     0.009132    0.001423   \n",
       "3    {filler}        cleft     embedded_wh_finite     0.089693    0.078533   \n",
       "4    {filler}        cleft  embedded_wh_nonfinite     0.057269    0.068639   \n",
       "..        ...          ...                    ...          ...         ...   \n",
       "247    {verb}  wh_question  embedded_wh_nonfinite     0.211237    0.110127   \n",
       "248    {verb}  wh_question           pseudo_cleft     0.841505    0.244043   \n",
       "249    {verb}  wh_question          restricted_rc     0.224899    0.137247   \n",
       "250    {verb}  wh_question         topicalization     0.744543    0.252308   \n",
       "251    {verb}  wh_question            wh_question     0.809839    0.225532   \n",
       "\n",
       "                                           data_points  n_samples  \n",
       "0    [1.0, 0.9479801325466016, 0.8328432313463736, ...          4  \n",
       "1          [0.03466758061428266, 0.057610572646662375]          2  \n",
       "2         [0.010138166441087537, 0.008125406540241074]          2  \n",
       "3    [0.2003410966874174, 0.03012053132554714, 0.09...          4  \n",
       "4    [0.15624271390433356, 0.007943775647185933, 0....          4  \n",
       "..                                                 ...        ...  \n",
       "247  [0.2557628494508001, 0.06806425463981211, 0.19...          4  \n",
       "248  [1.0384959650777246, 0.6163913357317051, 0.644...          4  \n",
       "249  [0.23430738971416312, 0.06704753584215735, 0.1...          4  \n",
       "250  [0.9495505102545027, 0.5333496301381966, 0.519...          4  \n",
       "251  [1.0, 0.5566341806838216, 0.6827210821389671, ...          4  \n",
       "\n",
       "[252 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to do our network analysis. The transfer matrix serves as an adjacency matrix, and now we want to calculate a host of metrics at different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_centrality(adj_matrix):\n",
    "    G = nx.from_pandas_adjacency(adj_matrix, create_using=nx.DiGraph())\n",
    "    G.remove_edges_from(list(nx.selfloop_edges(G)))\n",
    "    \n",
    "    # Calculate centrality measures\n",
    "    out_centrality = nx.out_degree_centrality(G)\n",
    "    in_centrality = nx.in_degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    \n",
    "    return {\n",
    "        \"out_centrality\": out_centrality,\n",
    "        \"in_centrality\": in_centrality,\n",
    "        \"betweenness_centrality\": betweenness_centrality\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_control_threshold(df, parent_from, type = \"both\"):\n",
    "    relevant_controls = df[(df[\"parent_from\"] == parent_from) &\n",
    "                           (df[\"parent_to\"] == \"control\")]\n",
    "    relevant_controls_lexical = df[(df[\"parent_from\"] == parent_from) &\n",
    "                           (df[\"parent_to\"] == \"control_lexical\")]\n",
    "    \n",
    "    if type == \"lexical\":\n",
    "        return relevant_controls_lexical[\"mean_normal\"].max()\n",
    "    elif type == \"normal\":\n",
    "        return relevant_controls[\"mean_normal\"].max()\n",
    "    else:\n",
    "        return max(relevant_controls[\"mean_normal\"].max(), \n",
    "               relevant_controls_lexical[\"mean_normal\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(df, threshold = None):\n",
    "    adj_matrix = pd.DataFrame(0, index=df[\"parent_from\"].unique(), columns=df[\"parent_to\"].unique())\n",
    "    adj_matrix = adj_matrix.astype(float)\n",
    "\n",
    "    parents_from = pd.unique(df[\"parent_from\"])\n",
    "    parents_to = pd.unique(df[\"parent_to\"])\n",
    "\n",
    "    for pf in parents_from:\n",
    "        \n",
    "        max_control = get_control_threshold(df, pf)\n",
    "        \n",
    "        threshold = max_control if threshold is None else threshold\n",
    "\n",
    "        for pt in parents_to:\n",
    "            filtered_data = df[(df[\"parent_from\"] == pf) & \n",
    "                               (df[\"parent_to\"] == pt)]\n",
    "            adj_matrix.loc[pf, pt] = float(filtered_data[\"mean_normal\"].mean()) if filtered_data[\"mean_normal\"].mean() > threshold else float(0.0)\n",
    "    \n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_centrality_across_thresholds(pos_df, thresholds, from_parents):\n",
    "    \n",
    "    thresholds_list = []\n",
    "    out_centrality_values = {parent: [] for parent in from_parents}\n",
    "    in_centrality_values = {parent: [] for parent in from_parents}\n",
    "    betweenness_centrality_values = {parent: [] for parent in from_parents}\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        thresholds_list.append(threshold)\n",
    "        adj_matrix = create_adjacency_matrix(pos_df, threshold)\n",
    "        centrality_measures = calculate_centrality(adj_matrix)\n",
    "        for parent in from_parents:\n",
    "            out_centrality_values[parent].append(\n",
    "                centrality_measures['out_centrality'].get(parent, 0)\n",
    "            )\n",
    "            in_centrality_values[parent].append(\n",
    "                centrality_measures['in_centrality'].get(parent, 0)\n",
    "            )\n",
    "            betweenness_centrality_values[parent].append(\n",
    "                centrality_measures['betweenness_centrality'].get(parent, 0)\n",
    "            )\n",
    "    \n",
    "    return {\n",
    "        \"thresholds_list\": thresholds_list,\n",
    "        \"out_centrality_values\": out_centrality_values,\n",
    "        \"in_centrality_values\": in_centrality_values,\n",
    "        \"betweenness_centrality_values\": betweenness_centrality_values\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_centrality_measure(\n",
    "    thresholds_list, \n",
    "    centrality_values, \n",
    "    from_parents, \n",
    "    control_threshold_normal, \n",
    "    control_threshold_lexical,\n",
    "    title,\n",
    "    ylabel\n",
    "):\n",
    "    for parent in from_parents:\n",
    "        plt.plot(thresholds_list, centrality_values[parent], marker='o', label=parent)\n",
    "        # Mark control threshold values\n",
    "        plt.axvline(x=control_threshold_normal[parent], color='gray', linestyle='--', alpha=0.5)\n",
    "        plt.axvline(x=control_threshold_lexical[parent], color='lightgray', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_centrality_measures(\n",
    "    thresholds_list, \n",
    "    out_centrality_values, \n",
    "    in_centrality_values, \n",
    "    betweenness_centrality_values,\n",
    "    control_threshold_normal,\n",
    "    control_threshold_lexical,\n",
    "    from_parents,\n",
    "    pos\n",
    "):\n",
    "    # Create a figure with multiple subplots for each centrality measure\n",
    "    plt.figure(figsize=(18, 15))\n",
    "    \n",
    "    # Plot out-degree centrality\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plot_single_centrality_measure(\n",
    "        thresholds_list, \n",
    "        out_centrality_values, \n",
    "        from_parents, \n",
    "        control_threshold_normal, \n",
    "        control_threshold_lexical,\n",
    "        'Out-degree Centrality vs Threshold',\n",
    "        'Out-degree Centrality'\n",
    "    )\n",
    "    \n",
    "    # Plot in-degree centrality\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plot_single_centrality_measure(\n",
    "        thresholds_list, \n",
    "        in_centrality_values, \n",
    "        from_parents, \n",
    "        control_threshold_normal, \n",
    "        control_threshold_lexical,\n",
    "        'In-degree Centrality vs Threshold',\n",
    "        'In-degree Centrality'\n",
    "    )\n",
    "    \n",
    "    # Plot betweenness centrality\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plot_single_centrality_measure(\n",
    "        thresholds_list, \n",
    "        betweenness_centrality_values, \n",
    "        from_parents, \n",
    "        control_threshold_normal, \n",
    "        control_threshold_lexical,\n",
    "        'Betweenness Centrality vs Threshold',\n",
    "        'Betweenness Centrality'\n",
    "    )\n",
    "    \n",
    "    plt.suptitle(f'Centrality Measures for {pos}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_centrality_plot_for_pos(pos, transfer_df):\n",
    "    pos_df = transfer_df[transfer_df[\"pos\"] == pos]\n",
    "    \n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "\n",
    "    control_threshold_lexical = {\n",
    "        parent_from: get_control_threshold(pos_df, parent_from, \"lexical\") for parent_from in from_parents\n",
    "    }\n",
    "    control_threshold_normal = {\n",
    "        parent_from: get_control_threshold(pos_df, parent_from, \"normal\") for parent_from in from_parents\n",
    "    }\n",
    "    \n",
    "\n",
    "    centrality_data = collect_centrality_across_thresholds(pos_df, thresholds, from_parents)\n",
    "    \n",
    "    plot_centrality_measures(\n",
    "        centrality_data[\"thresholds_list\"],\n",
    "        centrality_data[\"out_centrality_values\"],\n",
    "        centrality_data[\"in_centrality_values\"],\n",
    "        centrality_data[\"betweenness_centrality_values\"],\n",
    "        control_threshold_normal,\n",
    "        control_threshold_lexical,\n",
    "        from_parents,\n",
    "        pos\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(centrality_values, thresholds, control_threshold = False):\n",
    "    # Calculate the area under the curve (AUC) for each centrality measure\n",
    "    auc_values = {}\n",
    "    for parent, values in centrality_values.items():\n",
    "        control_threshold = 0 if not control_threshold else get_control_threshold(transfer_df[transfer_df[\"pos\"] == pos], parent, \"both\")\n",
    "        valid_indices = [i for i, t in enumerate(thresholds) if t >= control_threshold]\n",
    "        \n",
    "        if valid_indices:\n",
    "            # Get the valid thresholds and corresponding values\n",
    "            valid_thresholds = [thresholds[i] for i in valid_indices]\n",
    "            valid_values = [values[i] for i in valid_indices]\n",
    "            \n",
    "            # Calculate AUC for the valid portion\n",
    "            auc = np.trapz(valid_values, valid_thresholds)\n",
    "        else:\n",
    "            auc = 0.0\n",
    "        auc_values[parent] = auc\n",
    "    return auc_values\n",
    "\n",
    "def plot_auc(auc_values, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for parent, auc in auc_values.items():\n",
    "        plt.bar(parent, auc)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Parent Construction')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set up our helper functions, we can get our AUC's, and save them as csv's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/cjg9tbld1z95vqhh4kmccp200000gn/T/ipykernel_43446/2393095140.py:14: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(valid_values, valid_thresholds)\n",
      "/var/folders/kx/cjg9tbld1z95vqhh4kmccp200000gn/T/ipykernel_43446/2393095140.py:14: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(valid_values, valid_thresholds)\n",
      "/var/folders/kx/cjg9tbld1z95vqhh4kmccp200000gn/T/ipykernel_43446/2393095140.py:14: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(valid_values, valid_thresholds)\n",
      "/var/folders/kx/cjg9tbld1z95vqhh4kmccp200000gn/T/ipykernel_43446/2393095140.py:14: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(valid_values, valid_thresholds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../results/generalization/out_degree_auc_1.4b.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/cjg9tbld1z95vqhh4kmccp200000gn/T/ipykernel_43446/2393095140.py:14: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(valid_values, valid_thresholds)\n",
      "/var/folders/kx/cjg9tbld1z95vqhh4kmccp200000gn/T/ipykernel_43446/2393095140.py:14: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(valid_values, valid_thresholds)\n",
      "/var/folders/kx/cjg9tbld1z95vqhh4kmccp200000gn/T/ipykernel_43446/2393095140.py:14: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(valid_values, valid_thresholds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../results/generalization/in_degree_auc_1.4b.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kx/cjg9tbld1z95vqhh4kmccp200000gn/T/ipykernel_43446/2393095140.py:14: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = np.trapz(valid_values, valid_thresholds)\n"
     ]
    }
   ],
   "source": [
    "if embedded:\n",
    "    num_panels = 10\n",
    "else:\n",
    "    num_panels = 5\n",
    "\n",
    "# CHANGE THIS TO TRUE IF YOU WANT TO USE CONTROL THRESHOLD (I.E. only keep values above the control)\n",
    "control_threshold = False\n",
    "\n",
    "# # Export CSVs with AUC data for each centrality type\n",
    "def save_auc_csv(centrality_key, filename):\n",
    "    records = []\n",
    "    for pos in unique_pos[:num_panels-1]:\n",
    "        data = collect_centrality_across_thresholds(\n",
    "            transfer_df[transfer_df[\"pos\"] == pos],\n",
    "            np.linspace(0, 2, 101),\n",
    "            from_parents\n",
    "        )\n",
    "        auc_vals = get_auc(data[f\"{centrality_key}_values\"], data[\"thresholds_list\"], control_threshold)\n",
    "        max_v = max(auc_vals.values()) if auc_vals else 0\n",
    "        for parent, auc in auc_vals.items():\n",
    "            norm = (auc / max_v) if max_v > 0 else auc\n",
    "            records.append({\n",
    "                \"position\": pos,\n",
    "                \"parent\": parent,\n",
    "                \"auc\": auc,\n",
    "                \"normalized_auc\": norm\n",
    "            })\n",
    "    pd.DataFrame(records).to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "# Generate CSVs\n",
    "label = \"\" if not embedded else \"embedded_\"\n",
    "\n",
    "save_auc_csv(\"out_centrality\", f\"../results/generalization/{label}out_degree_auc_{size}.csv\")\n",
    "save_auc_csv(\"in_centrality\", f\"../results/generalization/{label}in_degree_auc_{size}.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
