{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from conllu import parse_incr\n",
    "import os, csv, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_words = {\"what\", \"who\", \"whom\", \"whose\", \"which\", \"where\", \"when\", \"why\", \"how\"}\n",
    "allowed_clause_deprels = {\"ccomp\", \"xcomp\", \"csubj\", \"obj\"}\n",
    "RELATIVIZERS = {\"who\", \"whom\", \"whose\", \"which\", \"when\", \"where\", \"why\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_embedded_wh(tokenlist):\n",
    "    \"\"\"\n",
    "    Detects embedded wh-questions in a CoNLL-U formatted sentence.\n",
    "\n",
    "    This version is embedding verb agnostic, meaning it identifies structures\n",
    "    based on syntactic patterns rather than a predefined list of embedding verbs.\n",
    "    It checks if a wh-clause (identified by allowed_clause_deprels) is\n",
    "    governed by any verb.\n",
    "\n",
    "    Args:\n",
    "        tokenlist (list): A list of dictionaries, where each dictionary represents\n",
    "                          a token with CoNLL-U fields like \"id\", \"form\", \"lemma\",\n",
    "                          \"head\", \"deprel\", \"upos\".\n",
    "        wh_words (set): A set of wh-words (e.g., \"what\", \"who\").\n",
    "        allowed_clause_deprels (set): A set of dependency relations that an\n",
    "                                      embedded wh-clause might have relative to its\n",
    "                                      governing (embedding) verb.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if an embedded wh-question matching the criteria is found,\n",
    "              False otherwise.\n",
    "    \"\"\"\n",
    "    if not tokenlist:\n",
    "        return False\n",
    "\n",
    "    token_map = {t[\"id\"]: t for t in tokenlist if isinstance(t, dict) and \"id\" in t}\n",
    "\n",
    "    for wh_token_index, token in enumerate(tokenlist):\n",
    "        \n",
    "        if not isinstance(token, dict):\n",
    "            continue\n",
    "\n",
    "        # Check wh-word\n",
    "        token_form = token.get(\"form\", \"\").lower()\n",
    "        if token_form not in wh_words:\n",
    "            continue\n",
    "\n",
    "        # Find wh-word root\n",
    "        current_head_id = token.get(\"head\")\n",
    "        wh_clause_verb = None\n",
    "        temp_current_id = current_head_id\n",
    "        while temp_current_id and temp_current_id in token_map:\n",
    "            candidate_node = token_map[temp_current_id]\n",
    "            if candidate_node.get(\"upos\") in {\"VERB\", \"AUX\"}:\n",
    "                wh_clause_verb = candidate_node\n",
    "                break\n",
    "            temp_current_id = candidate_node.get(\"head\")\n",
    "\n",
    "        if not wh_clause_verb:\n",
    "            continue\n",
    "\n",
    "        # Make sure deprel with head is a clausal complement, subject, or object.\n",
    "        wh_clause_deprel = wh_clause_verb.get(\"deprel\", \"\")\n",
    "        if wh_clause_deprel not in allowed_clause_deprels:\n",
    "            continue\n",
    "\n",
    "        # Find the embedding token, make sure it is a verb\n",
    "        embedding_verb_id = wh_clause_verb.get(\"head\")\n",
    "        if not embedding_verb_id or embedding_verb_id not in token_map:\n",
    "            continue\n",
    "        \n",
    "        embedding_verb_token = token_map[embedding_verb_id]\n",
    "        if embedding_verb_token.get(\"upos\") not in {\"VERB\", \"AUX\"}:\n",
    "            continue\n",
    "\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_matrix_wh(tokenlist):\n",
    "    \"\"\"\n",
    "    Determines if a sentence contains a wh-question in the matrix clause.\n",
    "    Args:\n",
    "        tokenlist: A list of token dictionaries from a CoNLL-U parse\n",
    "    Returns:\n",
    "        bool: True if the sentence contains a matrix wh-question, False otherwise\n",
    "    \"\"\"\n",
    "    if not tokenlist:\n",
    "        return False\n",
    "    \n",
    "    # Check if the sentence has a question mark\n",
    "    has_question_mark = any(\n",
    "        isinstance(token, dict) and token.get(\"form\") == \"?\" for token in tokenlist\n",
    "    )\n",
    "    if not has_question_mark:\n",
    "        return False\n",
    "\n",
    "    token_map = {t.get(\"id\"): t for t in tokenlist if isinstance(t, dict) and \"id\" in t}\n",
    "\n",
    "    # Find root\n",
    "    root_token = None\n",
    "    for token in tokenlist:\n",
    "        if isinstance(token, dict) and token.get(\"head\") == 0:\n",
    "            root_token = token\n",
    "            break\n",
    "    if not root_token:\n",
    "        return False\n",
    "\n",
    "    # Look for wh-word that is a direct dependent of the root (main clause)\n",
    "    for token in tokenlist:\n",
    "        if not isinstance(token, dict) or \"form\" not in token:\n",
    "            continue\n",
    "        if token.get(\"form\", \"\").lower() in wh_words:\n",
    "            if token.get(\"id\") == root_token.get(\"id\"):\n",
    "                return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cleft(tokenlist):\n",
    "    \"\"\"\n",
    "    Detect if a sentence in CoNLL-U format is an it-cleft construction.\n",
    "    Returns:\n",
    "        dict: Information about whether the sentence is an it-cleft and its components.\n",
    "\n",
    "    Based off https://universaldependencies.org/en/dep/acl-relcl.html#clefts\n",
    "    \"\"\"\n",
    "    if not tokenlist:\n",
    "        return {\"is_it_cleft\": False}\n",
    "\n",
    "    # Look for \"it\" as an expletive\n",
    "    it_token = next((token for token in tokenlist if token.get(\"form\", \"\").lower() == \"it\" and token.get(\"deprel\") == \"expl\"), None)\n",
    "    if not it_token:\n",
    "        return {\"is_it_cleft\": False}\n",
    "\n",
    "    # Look for a copula verb (e.g., \"is\", \"was\") associated with \"it\"\n",
    "    copula_token = next((token for token in tokenlist if token.get(\"deprel\") == \"cop\"), None)\n",
    "    if not copula_token:\n",
    "        return {\"is_it_cleft\": False}\n",
    "\n",
    "    # Look for a relative clause (advcl:relcl) dependent on the copula or another token in the sentence\n",
    "    rel_clause_token = next((token for token in tokenlist if token.get(\"deprel\") == \"advcl:relcl\"), None)\n",
    "    if not rel_clause_token:\n",
    "        return {\"is_it_cleft\": False}\n",
    "\n",
    "    # Check for a relativizer (e.g., \"that\", \"who\") associated with the relative clause\n",
    "    relativizer_token = next((token for token in tokenlist if token.get(\"head\") == rel_clause_token.get(\"id\") and token.get(\"deprel\") == \"mark\"), None)\n",
    "\n",
    "    return {\n",
    "        \"is_it_cleft\": True,\n",
    "        \"it_token\": it_token,\n",
    "        \"copula_token\": copula_token,\n",
    "        \"rel_clause_token\": rel_clause_token,\n",
    "        \"relativizer_token\": relativizer_token\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pseudocleft(tokenlist):\n",
    "    \"\"\"\n",
    "    Determines if a sentence contains a pseudocleft construction.\n",
    "    \n",
    "    Based off: https://universaldependencies.org/en/dep/acl-relcl.html#clefts\n",
    "    \n",
    "    Args:\n",
    "        tokenlist: A list of token dictionaries from a CoNLL-U parse\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the sentence contains a pseudocleft construction, False otherwise\n",
    "    \"\"\"\n",
    "    # Check if we have tokens to process\n",
    "    if not tokenlist:\n",
    "        return False\n",
    "    \n",
    "    # Build a map of token IDs to tokens for easy lookup\n",
    "    token_map = {t.get(\"id\"): t for t in tokenlist if isinstance(t, dict) and \"id\" in t}\n",
    "\n",
    "    \n",
    "    # find the root token (the one whose head is 0)\n",
    "    root_token = next(\n",
    "        (t for t in tokenlist \n",
    "         if isinstance(t, dict) and t.get(\"head\") == 0),\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    # Find a copula whose head is the root\n",
    "    copula = next(\n",
    "        (t for t in tokenlist\n",
    "         if isinstance(t, dict)\n",
    "           and t.get(\"deprel\") == \"cop\"\n",
    "           and t.get(\"head\") == root_token.get(\"id\")),\n",
    "        None\n",
    "    )\n",
    "    if not copula:\n",
    "        return False\n",
    "\n",
    "    # Find a subject whose head is the root\n",
    "    subj = next(\n",
    "        (t for t in tokenlist\n",
    "         if isinstance(t, dict)\n",
    "           and t.get(\"deprel\") == \"nsubj\"\n",
    "           and t.get(\"head\") == root_token.get(\"id\")\n",
    "           and t.get(\"id\") <= copula.get(\"id\")\n",
    "           and t.get(\"form\", \"\").lower() in wh_words),\n",
    "        None\n",
    "    )\n",
    "    if not subj:\n",
    "        return False\n",
    "\n",
    "    # Find a relative clause (acl:relcl) whose head is that subject\n",
    "    relcl = next(\n",
    "        (t for t in tokenlist\n",
    "         if isinstance(t, dict)\n",
    "           and t.get(\"deprel\") == \"acl:relcl\"\n",
    "           and t.get(\"head\") == subj.get(\"id\")),\n",
    "        None\n",
    "    )\n",
    "    return relcl is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_topicalization(tokenlist):\n",
    "    \"\"\"\n",
    "    Detects argument topicalization (not adjuncts) in a sentence.\n",
    "    Returns True if a core argument (obj, iobj, etc.) appears sentence-initially,\n",
    "    possibly followed by a comma, and is not a subject or adjunct.\n",
    "    \"\"\"\n",
    "    if not tokenlist:\n",
    "        return False\n",
    "\n",
    "    # TOpicalization is marked by dislocation in UD\n",
    "    if not \"dislocated\" in [tokenlist[i].get(\"deprel\", \"\") for i in range(len(tokenlist))]:\n",
    "        return False\n",
    "    \n",
    "    # Make sure that the dislocared item is before the head, and not an adposition (would indicate adjunct)\n",
    "    for i, token in enumerate(tokenlist):\n",
    "        if not isinstance(token, dict) or \"form\" not in token:\n",
    "            continue\n",
    "\n",
    "        if \"dislocated\" in token.get(\"deprel\"):\n",
    "            head = token.get(\"head\")\n",
    "            if token.get(\"id\") < head:\n",
    "                if token.get(\"upos\") == \"ADP\":\n",
    "                    continue\n",
    "                return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_restrictive_relative(tokenlist):\n",
    "    \"\"\"\n",
    "    Returns True if a restrictive relative clause with a relativizer is found, else False.\n",
    "    \"\"\"\n",
    "    for t in tokenlist:\n",
    "        # Check if a relatibe clase\n",
    "        if t.get(\"deprel\") == \"acl:relcl\":\n",
    "            relcl_idx = t[\"id\"]\n",
    "            head_idx = t.get(\"head\")\n",
    "            if head_idx is None or head_idx <= 0:\n",
    "                continue\n",
    "\n",
    "            # check for a relativizer \n",
    "            has_relativizer = any(\n",
    "                ref.get(\"form\", \"\").lower() in RELATIVIZERS and ref.get(\"head\") == t[\"id\"]\n",
    "                for ref in tokenlist\n",
    "            )\n",
    "            if not has_relativizer:\n",
    "                continue  # Not a real relative clause\n",
    "\n",
    "            # make sure no comma (comma is likely ascriptive)\n",
    "            start = min(head_idx, relcl_idx)\n",
    "            end = max(head_idx, relcl_idx)\n",
    "            has_comma = any(\n",
    "                token[\"form\"].strip() == \",\" and start < token[\"id\"] < end\n",
    "                for token in tokenlist\n",
    "            )\n",
    "\n",
    "            if not has_comma:\n",
    "                return True \n",
    "\n",
    "    return False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 16622\n",
      "Total matrix wh-questions: 82, proportion: 0.49%\n",
      "Total embedded wh-questions: 308, proportion: 1.85%\n",
      "Total cleft constructions: 20, proportion: 0.12%\n",
      "Total pseudo-cleft constructions: 6, proportion: 0.04%\n",
      "Total topicalization constructions: 6, proportion: 0.04%\n",
      "Total restrictive relative clauses: 504, proportion: 3.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ud_folder = \"ud/\"\n",
    "if not os.path.exists(ud_folder):\n",
    "    os.makedirs(ud_folder, exist_ok=True)\n",
    "    print(f\"Please download the UD 2.16 from https://universaldependencies.org/ and to folder '{ud_folder}'.\")\n",
    "    exit(1)\n",
    "\n",
    "ud_file_paths = [ud_folder + path for path in\n",
    "                [\"ud-treebanks-v2.15/UD_English-EWT/en_ewt-ud-train.conllu\",\n",
    "                 \"ud-treebanks-v2.15/UD_English-EWT/en_ewt-ud-dev.conllu\",\n",
    "                 \"ud-treebanks-v2.15/UD_English-EWT/en_ewt-ud-test.conllu\"]]\n",
    "\n",
    "# Counters and example lists\n",
    "total_sentences = 0\n",
    "total_matrix = 0\n",
    "total_embedded = 0\n",
    "total_cleft = 0\n",
    "total_pseudo_cleft = 0\n",
    "total_topicalization = 0\n",
    "total_restrictive_relative = 0\n",
    "\n",
    "ud_examples_matrix = []\n",
    "ud_examples_embedded = []\n",
    "ud_examples_cleft = []\n",
    "ud_examples_pseudo_cleft = []\n",
    "ud_examples_topicalization = []\n",
    "ud_examples_restrictive = []\n",
    "\n",
    "for ud_file_path in tqdm(ud_file_paths):\n",
    "    # Process the UD corpus.\n",
    "    with open(ud_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for tokenlist in parse_incr(f):\n",
    "            total_sentences += 1\n",
    "            sentence_text = \" \".join(token[\"form\"] for token in tokenlist if token[\"form\"])\n",
    "            \n",
    "            if is_matrix_wh(tokenlist):\n",
    "                total_matrix += 1\n",
    "                ud_examples_matrix.append(sentence_text)\n",
    "            if is_embedded_wh(tokenlist):\n",
    "                total_embedded += 1\n",
    "                ud_examples_embedded.append(sentence_text)\n",
    "            if is_cleft(tokenlist)['is_it_cleft']:\n",
    "                total_cleft += 1\n",
    "                ud_examples_cleft.append(sentence_text)\n",
    "            if is_pseudocleft(tokenlist):\n",
    "                total_pseudo_cleft += 1\n",
    "                ud_examples_pseudo_cleft.append(sentence_text)\n",
    "            if is_topicalization(tokenlist):\n",
    "                total_topicalization += 1\n",
    "                ud_examples_topicalization.append(sentence_text)\n",
    "            if is_restrictive_relative(tokenlist):\n",
    "                total_restrictive_relative += 1\n",
    "                ud_examples_restrictive.append(sentence_text)\n",
    "\n",
    "# Write the example sentences to CSV files.\n",
    "os.makedirs(\"ud/ud_frequency\", exist_ok=True)\n",
    "\n",
    "def write_examples(file_path, examples):\n",
    "    with open(file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"sentence\"])\n",
    "        for ex in examples:\n",
    "            writer.writerow([ex])\n",
    "\n",
    "write_examples(\"ud_frequency/matrix.csv\", ud_examples_matrix)\n",
    "write_examples(\"ud_frequency/embedded.csv\", ud_examples_embedded)\n",
    "write_examples(\"ud_frequency/cleft.csv\", ud_examples_cleft)\n",
    "write_examples(\"ud_frequency/pseudo_cleft.csv\", ud_examples_pseudo_cleft)\n",
    "write_examples(\"ud_frequency/topicalization.csv\", ud_examples_topicalization)\n",
    "write_examples(\"ud_frequency/restrictive_relative.csv\", ud_examples_restrictive)\n",
    "\n",
    "print(f\"Total sentences: {total_sentences}\")\n",
    "print(f\"Total matrix wh-questions: {total_matrix}, proportion: {total_matrix/total_sentences:.2%}\")\n",
    "print(f\"Total embedded wh-questions: {total_embedded}, proportion: {total_embedded/total_sentences:.2%}\")\n",
    "print(f\"Total cleft constructions: {total_cleft}, proportion: {total_cleft/total_sentences:.2%}\")\n",
    "print(f\"Total pseudo-cleft constructions: {total_pseudo_cleft}, proportion: {total_pseudo_cleft/total_sentences:.2%}\")\n",
    "print(f\"Total topicalization constructions: {total_topicalization}, proportion: {total_topicalization/total_sentences:.2%}\")\n",
    "print(f\"Total restrictive relative clauses: {total_restrictive_relative}, proportion: {total_restrictive_relative/total_sentences:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
